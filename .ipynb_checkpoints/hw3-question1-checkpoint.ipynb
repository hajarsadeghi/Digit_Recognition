{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92c0fc5",
   "metadata": {},
   "source": [
    "# Question 1.\n",
    "Implement a two-layer perceptron with the backpropagation algorithm to solve the parity problem.\n",
    "You must implement the forward and backpropagation paths entirely on your own,\n",
    "including an implementation of a perceptron and the activation function. The desired\n",
    "output for the parity problem is 1 if an input pattern (which contains 4-binary bits) contains an odd\n",
    "number of 1's, and 0 otherwise. Follow the algorithm introduced in class. The learning procedure\n",
    "is stopped when an absolute error (diâ€€erence) of 0.05 is reached for every input pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad003c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2d66687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# parity_dataset = {'data' :  ['0000', '0001', '0010', '0011', '0100', '0101', '0110', '0111', '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111'],\n",
    "#                   'labels':  [0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0]}\n",
    "# np.array(parity_dataset['data'])\n",
    "\n",
    "X = np.array([[0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], \n",
    "                  [0, 0, 1, 1], [0, 1, 0, 0], [0, 1, 0, 1], \n",
    "                  [0, 1, 1, 0], [0, 1, 1, 1], [1, 0, 0, 0], \n",
    "                  [1, 0, 0, 1], [1, 0, 1, 0], [1, 0, 1, 1],\n",
    "                  [1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 1, 0], \n",
    "                  [1, 1, 1, 1]])\n",
    "print(X.shape)\n",
    "y = np.array([[0], [1], [1], \n",
    "                  [0], [1], [0], \n",
    "                  [0], [1], [1], \n",
    "                  [0], [0], [1],\n",
    "                  [0], [1], [1], \n",
    "                  [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554de5d1",
   "metadata": {},
   "source": [
    "As we have four (four bits) inputs, we will use this activation potential, where X1-X4 are our input values, w1-w4 are our weight values, and bias.\n",
    "\n",
    "activation = (w1 * X1) + (w2 * X2) + (w3 * X3) + (w4 * X4) + bias\n",
    "\n",
    "We will use the sigmoid function as our activatation function\n",
    "\n",
    "a(z) = 1/ 1 + e^-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4fe0b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key hyperparameters\n",
    "class hyperparam:\n",
    "    inputLayerSize = 4\n",
    "    outputLayerSize = 1\n",
    "    hiddenLayerSize = 1\n",
    "    num_hid_units = 4\n",
    "    bias = np.random.randn\n",
    "    \n",
    "    lr_rate = 0.05 #Learning Rate\n",
    "    num_epochs = 50 #Number of epochs\n",
    "    \n",
    "    momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3905b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DNN(object):\n",
    "#     def __init__(self):\n",
    "# #         super(DNN, self).__init__()\n",
    "        \n",
    "#         self.in_dim = hyperparam.inputLayerSize\n",
    "#         self.out_dim = hyperparam.outputLayerSize\n",
    "#         self.hidden_dim = hyperparam.hiddenLayerSize\n",
    "#         self.hid_units = hyperparam.num_hid_units\n",
    "        \n",
    "#         #Weights random.randint(1,11)\n",
    "#         self.W1 = np.random.randn(self.in_dim, self.hid_units)\n",
    "#         print(self.W1, 'W1')\n",
    "#         print()\n",
    "#         self.W2 = np.random.randn(self.hid_units, self.out_dim)\n",
    "        \n",
    "\n",
    "#     #make predictions\n",
    "#     def forward(self, X):\n",
    "#         #propagate inputs through network\n",
    "\n",
    "#         #compute the activation potential for the hidden layer\n",
    "#         self.z2 = np.dot(X, self.W1) + self.B1\n",
    "#         print(self.z2, 'z2')\n",
    "#         #feed the activation potential to our activation function which is a sigmoid function in this case\n",
    "#         self.a2 = self.sigmoid(self.z2)\n",
    "# #         print(self.a2, 'a2')\n",
    "#         #compute the activation potential for the output layer\n",
    "#         self.z3 = np.dot(self.a2, self.W2)\n",
    "# #         print(self.z3, 'z3')\n",
    "#         yHat = self.sigmoid(self.z3)\n",
    "#         print(yHat, 'yHat')\n",
    "\n",
    "#         return yHat\n",
    "\n",
    "#     def sigmoid(self, z):\n",
    "#         #apply sigmoid activation function to activation potential\n",
    "#         return 1/ (1+np.exp(-z))\n",
    "    \n",
    "#     def back(self, X, y, epoch, lr):\n",
    "#         #update weight & bias\n",
    "#         for ep in range(epoch):\n",
    "#             i=0\n",
    "#             while(i<len(y)):\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7fa9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "t0,t1 = 1,1000000 # learning schedule hyperparams\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "def cost_function(x,y,B):\n",
    "    predictions = np.dot(x,B.T)\n",
    "    cost = (1/len(y)) * np.sum((predictions - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def stochastic_GD(X, y, ep, delta, momentum):\n",
    "    #X = np.c_[np.ones((X.shape[0],1)),X]\n",
    "    m = X.shape[0]\n",
    "    #theta = np.zeros(X.shape[1])\n",
    "    theta = slef.Weights\n",
    "    mse = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            random_index= np.random.randint(m)\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "            eta = learning_schedule(epoch * m + i)\n",
    "            theta = theta - eta * gradients\n",
    "            mse = cost_function(xi,yi,theta)           \n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self,Layers = [4, 8, 1]):\n",
    "#       super(DNN, self).__init__()        \n",
    "#         self.in_dim = hyperparam.inputLayerSize\n",
    "#         self.out_dim = hyperparam.outputLayerSize\n",
    "#         self.hidden_dim = hyperparam.hiddenLayerSize\n",
    "#         self.hid_units = hyperparam.num_hid_units\n",
    "        \n",
    "        self.Layers = Layers\n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        \n",
    "        #Set Random Weights and Biases except for the output and save them for updtaes in backpropagation\n",
    "        for i in range(len(Layers)-1):\n",
    "            self.Weights.append(np.random.uniform(low = -1, high = 1, size = (self.Layers[i],self.Layers[i+1])))\n",
    "            self.Biases.append(np.random.uniform(low = -1, high = 1, size = (1, self.Layers[i+1])))\n",
    "\n",
    "    #Make predictions\n",
    "    def forward(self, X):\n",
    "        act_pot = []\n",
    "        x_layer = np.copy(X)\n",
    "        output = [x_layer]\n",
    "    \n",
    "        for i in range(len(self.Weights)):\n",
    "#            #print(\"i \", i)\n",
    "#             print(\"self.Weights[i].shape \", self.Weights[i].shape)\n",
    "#             print(\"x_layer.shape \",x_layer.shape)\n",
    "#            #print(\"x_layer\", x_layer)\n",
    "#             print(\"self.Biases[i].shape\", self.Biases[i].shape)\n",
    "            act_pot.append(x_layer.dot(self.Weights[i]) + self.Biases[i])\n",
    "            x_layer = self.Sigmoid(act_pot[-1])\n",
    "            output.append(x_layer)\n",
    "        return (act_pot, output)\n",
    "\n",
    "    def Sigmoid(self, z):\n",
    "        #apply sigmoid activation function to activation potential\n",
    "        return 1/ (1+np.exp(-z))\n",
    "    \n",
    "    def backpropagation(self, X, y, act_pot, output):\n",
    "        deltas = [None] * len(self.Weights)\n",
    "        #Compute delta for output\n",
    "        deltas[-1] = (y - output[-1])*self.sigmoidDrv(act_pot[-1])\n",
    "        #Compute delta for the rest of layers in reversed order\n",
    "        for i in reversed(range(len(self.Weights)-1)):\n",
    "            deltas[i] = deltas[i+1].dot(self.Weights[i+1].T) *self.sigmoidDrv(act_pot[i]) \n",
    "        return deltas\n",
    "        \n",
    "    def update_W (self, X, output, delta):\n",
    "        lr = 0.01\n",
    "        for i in range(len(self.Weights)):\n",
    "            input= X\n",
    "            if i != 0:\n",
    "                input = output[i]\n",
    "            self.Weights[i] = self.Weights[i] - (lr*delta[i].T.dot(input)).T\n",
    "            self.Biases[i] = self.Biases[i] - lr*delta[i]\n",
    "            \n",
    "    def train(self, X, y, epoch = 50, lr = 0.05):\n",
    "        error = np.zeros(epoch)\n",
    "        #mse = 0\n",
    "        #update weight & bias\n",
    "        for ep in range(epoch):\n",
    "            combined = list(zip(X,y))\n",
    "            random.shuffle(combined)\n",
    "            X[:], y[:] = zip(*combined)\n",
    "            i=0\n",
    "            while(i<len(y) or any(er for er in errors if er>0.05)):\n",
    "                act_pot, yHat = self.forward(X)\n",
    "                error = abs(y - yHat[-1])\n",
    "                delta = self.backpropagation(X, y, act_pot, yHat)\n",
    "                self.update_W (X,yHat, delta)\n",
    "\n",
    "    def sigmoidDrv(self, z):\n",
    "        return self.Sigmoid(z)*(1- self.Sigmoid(z))\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac03b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DNN()\n",
    "#NN.forward(X)\n",
    "# p, yh = NN.forward(X)\n",
    "# d = NN.backpropagation(X, y, p, yh)\n",
    "# NN.update_W(X, yh, d)\n",
    "NN.train(X,y, epoch=60, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfd439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72bc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
